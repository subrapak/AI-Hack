{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Brooklyn House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Runtime - 2 hours. An entire hour goes in one cell.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook below shows a project completed during Imperial College London's inaugral 'AI Hack' in November 2018. The challenge was to, given a dataset with a number of samples and features, predict current house prices for the Brooklyn area.\n",
    "A number of data cleaning techniques were used as much of the data was unrecorded, or unusable (e.g. having a year of sale as 0), which had to be accounted for.\n",
    "Once data cleaning was done, the non-numerical data (e.g. categorical variables) were removed, and of the remaining variables, even more were discarded based on the author's discretion. \n",
    "\n",
    "Finally, with a usable dataset, a number of machine learning techniques were used to estimate house prices on a validation set, and the performance of each technique was measured by the R^2 value between the predicted and actual sale price for each sample in the validation dataset. The best performing method was the RandomForestRegressor model provided in the sklearn library, with an R$^2$ value just under 0.8, and outperformed all other models comfortably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First ensure we're in the right directory. Then import all relevant modules, after which, load all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/arohan/Documents/CODING/AI_Hack/brooklyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls #checking we're in the right folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas for dataframe processing\n",
    "import pandas as pd\n",
    "\n",
    "#import numpy\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "#import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#set warnings off\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import sklearn for the ML\n",
    "import sklearn\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the raw data from the .csv file\n",
    "df_trainvalid_raw = pd.read_csv(\"brooklyntrainset.csv\") #training dataset, to be split into train and test\n",
    "df_testraw = pd.read_csv(\"brooklyntestset.csv\") #validation dataset\n",
    "pd.set_option('display.max_columns', 111) #viewing preference, max columns visible\n",
    "print(df_trainvalid_raw.shape)\n",
    "print(df_testraw.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that they have different numbers of rows - expected, as obviously the validation set isn't going to be as large as the training set. However, the training set has one more column than the test set, why? And which column is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traval_cols = df_trainvalid_raw.columns.values\n",
    "test_cols = df_testraw.columns.values\n",
    "\n",
    "#print the col which isn't in test_cols\n",
    "for name in traval_cols:\n",
    "    if name not in test_cols:\n",
    "        print(name)\n",
    "    \n",
    "#df_trainvalid_raw['sale_price'][df_trainvalid_raw['sale_price'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra column is the dependent variable/label/what we're trying to predict, sale_price. Looking at the dataset shows us that a lot of rows in the training set have a sale_price of 0 (uncomment the bottom line above) - this is useless, so get rid of all rows that have a sale_price of 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with sale_price = 0 and NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the training/validation and test data\n",
    "df_all_data_raw = pd.concat([df_trainvalid_raw, df_testraw],ignore_index=True)\n",
    "print(\"Number of all_data rows with 'sale_price=0' included: \",len(df_all_data_raw))\n",
    "df_all_data = df_all_data_raw.drop(df_all_data_raw.loc[df_all_data_raw['sale_price'] == 0].index,axis=0)\n",
    "print(\"Number of all_data rows after 'sale_price=0' rows dropped: \", len(df_all_data)) \n",
    "\n",
    "n_data = len(df_all_data) #number of data points for training/testing\n",
    "n_data_valid = len(df_testraw) #number of test points for validation (where sale_price = np.NaN)\n",
    "n_data_train = n_data - n_data_valid\n",
    "#df_all_data.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon further inspection of the data, we find there are a lot of **features** (i.e. columns) with many unrecorded values - NaNs. These are not useful so we get rid of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_na is a df with where every value is mapped to true if null and false otherwise\n",
    "#Summing it sums up all the true/falses in each col, and then it's turned into a %\n",
    "\n",
    "df_all_na=(df_all_data.isnull().sum() / len(df_all_data)) *100 #all_na is 112 long\n",
    "len(df_all_na[df_all_na==0]) #17 columns with no NAs\n",
    "\n",
    "#Drop the (17) columns that have no NAs, below:\n",
    "df_all_na=df_all_na.drop(df_all_na[df_all_na==0].index).sort_values(ascending=False)\n",
    "\n",
    "#Now we have 112 - 17 = 95 columns in the all_na array\n",
    "#Getting rid of 20 columns with the most NAs, from all_data\n",
    "df_data_clean = df_all_data.drop(df_all_na[0:20].index,axis=1) \n",
    "\n",
    "#This should leave us with 112-20 = 92 columns. Check using len(list(all_data_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've dropped the 20 columns with the most NaNs - however, there are still many NaN values in other columns. We deal with this by 'imputing' the data - replacing the NaN value with a suitable alternative. For some columns, this means replacing NaNs with the average value from the rest of the data set. \n",
    "\n",
    "Also notice: because the **label** (Y value) is ``sale_price``, which is a float, it is in the df_num2_withy dataframe below. We create this so that we can drop the ``sale_price`` column from it, so that when imputing missing values later on we don't fill in values for sale_price - this would ruin the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into different variable types\n",
    "#integer type columns\n",
    "df_num1 = df_data_clean[df_data_clean.select_dtypes(include=['int64']).columns]\n",
    "num1_colnames = list(df_num1) #need to save column names for later\n",
    "\n",
    "#float type columns\n",
    "df_num2_withy = df_data_clean[df_data_clean.select_dtypes(include=['float64']).columns]\n",
    "df_num2 = all_data_num2_withy.drop('sale_price',axis = 'columns',inplace=False)\n",
    "num2_colnames = list(df_num2) #need to save column names for later\n",
    "\n",
    "#object type columns\n",
    "all_data_obj = df_data_clean[df_data_clean.select_dtypes(include=['object']).columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the imputer, and impute all the NaN values in all three dataframes that we have made.\n",
    "\n",
    "The imputer function works based on numerical values (as it imputes missing values with the mean) therefore it can't be used for the object columns, and we take a different approach for those. First, drop any columns that seem inconsequential to the model and that we can't hope to take any data from. Then just replace any NaN values with an empty string, ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputer() #create imputer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer(axis=0,copy=True,missing_values=(np.nan),strategy='mean')\n",
    "\n",
    "imp.fit(df_num1)\n",
    "all_data_num1 = imp.transform(df_num1)\n",
    "imp.fit(df_num2)\n",
    "all_data_num2 = imp.transform(all_data_num2)\n",
    "\n",
    "#Drop useless object columns \n",
    "drop_objs = ['Address','OwnerName','Borough','Version','address','building_class_category','neighborhood']\n",
    "all_data_obj_dropped = all_data_obj.drop(drop_objs,axis = 'columns',inplace=False)\n",
    "\n",
    "#Replace remaining NaNs with empty strings\n",
    "all_data_obj_dropped.replace(np.nan,'',regex=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now usually, to convert the remaining categorical variables into useful data points, we have to 'one hot encode' them. This means first assign a numerical value for each categorical variable. This works in some cases but mostly it implies that certain valeus are inherently better than others - which is often untrue for categorical values.\n",
    "\n",
    "So, to compensate for this, one hot encoding takes place. Assuming there are 5 possible categorical values (e.g. colours red, blue, green, yellow, pink) and there are 300 total data points (e.g. 300 people surveyed on their favourite colour), then each person's colour choice becomes an array of length 5, with one '1' corresponding to their favourite colour and the rest of the array filled with zeros.\n",
    "\n",
    "Example:\n",
    "    Tony - red,\n",
    "    Steve - blue,\n",
    "    Bruce - green,\n",
    "would become:\n",
    "    Tony - [1,0,0],\n",
    "    Steve - [0,1,0],\n",
    "    Bruce - [0,0,1]\n",
    "    \n",
    "Ideally this is what we should do with all the categorical variables. However if we do this, our dataset just becomes stupidly big. For example, there are over 100 types of 'FireComp', which is just ONE variable, and there are 12 of these variables, and 250,000 data points for each. Let's leave that for now and stick to the numerical data that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting Sparse rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we found from the data is there are many 'sparse' rows and columns, AKA a lot of rows and columns mostly filled with zeros. These haven't been picked up by the imputer, as it only imputes values that are NaNs (which zeros don't count as). We make the decision to get rid of all rows with more than a certain threshold of zeros.\n",
    "\n",
    "This has to be done using arrays, as the data is now in list/array form after the imputing stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_thresh = 0.25 #if more than this fraction of values are zero, drop the row\n",
    "count = [] #to show how many rows have been dropped\n",
    "all_data_nums=[] #the new list we store our data in\n",
    "\n",
    "#Concatenate Y row with rest of data\n",
    "Y = np.reshape(np.array(all_data_num2_withy['sale_price']),[len(all_data_num1),1])\n",
    "temp_nums=np.concatenate((all_data_num1,all_data_num2,Y),axis=1) #imputed integer and float columns\n",
    "\n",
    "\n",
    "for i in range(len(temp_nums)):\n",
    "    if(np.count_nonzero(temp_nums[i] == 0) < (zero_thresh * (len(num1_colnames)+len(num2_colnames)))):\n",
    "        all_data_nums.append(temp_nums[i]) #append if few enough zeros\n",
    "    else:\n",
    "        count.append(np.count_nonzero(temp_nums[i]==0)) #append if too many zeros\n",
    "\n",
    "n_zerorows_dropped = len(count)\n",
    "print(n_zerorows_dropped,\"out of\",len(temp_nums),\"rows dropped due to too many zeros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to drop any *columns* with too many zeros. This is easiest to do when the data is converted back into dataframe format, so first convert the data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_nums = pd.DataFrame(all_data_nums,columns = num1_colnames+num2_colnames+['sale_price'])\n",
    "df_all_data_nums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then find the number of zeros in each column as a % of the total number of values in the column. Then remove all columns with less than a certain threshold of non-zero values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_na = (df_all_data.astype(bool).sum(axis=0) / len(df_all_data))*100\n",
    "\n",
    "#drops all columns with less than threshold% non-zero values\n",
    "threshold = 40\n",
    "sorted_na = show_na[show_na < threshold].sort_values(ascending=True)\n",
    "all_data_with_outliers = df_all_data.drop(sorted_na.index,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember at the start when we joined trainvalid_raw and test_raw data together?\n",
    "*(We did this so we could impute data for both of them at the same time)*\n",
    "\n",
    "Now, we should split the data into training/validation and test again. Then later, we can split the training/validation into training and validation data. We'll train our model on the training data and test it on the validation data. For the test data, we don't actually have the answers so we'll just leave that alone!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split all data into training/validation ('model') and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sale_nans = all_data_with_outliers['sale_price'].isnull().sum()\n",
    "n_data_points = len(all_data_with_outliers)\n",
    "n_usable = n_data_points - n_sale_nans\n",
    "print(n_usable)\n",
    "print(n_sale_nans)\n",
    "\n",
    "\n",
    "#Now we know that all_data_with_outliers is 196871 rows x 59 columns. We also know the first 167882 rows are training data and the next 28989 (n_sale_nans) rows are test data.\n",
    "#So let's split them up accordingly:\n",
    "\n",
    "#model refers to training and validation data together\n",
    "model_data = pd.DataFrame(all_data_with_outliers.iloc[:n_usable])\n",
    "test_data = pd.DataFrame(all_data_with_outliers.iloc[n_usable:])\n",
    "print('Number of features:',len(list(model_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are still 59 columns worth of indicators - not all of these will be useful. Although we have got rid of columns with too many NaNs and zeros, we now need to see what they actually mean and if they're useful or not. Here goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually remove unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_data['Unnamed: 0'] #what even is that, nope\n",
    "#model_data['X'] #what even is that, nope\n",
    "model_data['block'] #no NaNs, no zeros. Keep\n",
    "#model_data['borough'] #every borough is 3. nope\n",
    "#model_data['lot'] #lot of exceptions etc and not categorical or continuous data, nope\n",
    "model_data['residential_units'] #residential units in a tax lot. Has one crazy big outlier.\n",
    "model_data['tax_class_at_sale'] #categorical. Get rid of all rows with tax classes 3 and 4, non residential\n",
    "model_data['total_units'] #total units at the property. YES\n",
    "model_data['year_built'] #watch out for zeros. But Yes.\n",
    "model_data['year_of_sale'] #no zeros. YES\n",
    "#model_data['zip_code'] #same as tax lot. No thanks\n",
    "#model_data['AreaSource'] #to do with source files for tax lot allocation. nope\n",
    "model_data['AssessLand'] #assessed value of the land if vacant. YES\n",
    "model_data['AssessTot'] #assessed value for the total year. YES\n",
    "#model_data['BBL'] #borough, block and lot in one. nope\n",
    "model_data['BldgArea'] #total gross area in sqft\n",
    "model_data['BldgDepth'] #keep, only couple of horizontal outliers\n",
    "model_data['BldgFront'] #keep, only couple of horizontal outliers\n",
    "#model_data[BoroCode] #borough, nope\n",
    "model_data['BsmtCode'] #continuous, standard of basement, may as well\n",
    "#model_data[BuiltFAR]  #floor area as ratio of tax lot, no need (got BldgArea)\n",
    "#model_data['CB2010'] #census block, nope\n",
    "#model_data['CD'] #community district, categorical data, nope\n",
    "#model_data['CT2010'] #census tract, nope\n",
    "#model_data['Council'] #similar to borough, all refer to Brooklyn obvs, nope\n",
    "#model_data['ExemptLand'] #Land Exempt from tax. Nope\n",
    "#model_data[ExemptTot'] #Something else Exempt from tax. Nope\n",
    "#model_data['FacilFAR'] #max allowable floor area ratio? nope\n",
    "#model_data['HealthArea'] #Health Area code - categorical\n",
    "#model_data['HealthCent'] #Health centre code - categorical\n",
    "#model_data['LandUse'] #type of building - useful to get rid of non residential buildings. \n",
    "#model_data['LotArea'] #tax lot area\n",
    "#model_data['LotDepth'] #tax lot depth\n",
    "#model_data['LotFront'] #tax lot frontage\n",
    "#model_data['LotType'] #tax lot type \n",
    "#model_data['NumBldgs'] #num of bldgs in the tax lot\n",
    "#model_data['NumFloors'] #num of floors of highest building in tax lot\n",
    "#model_data['PLUTOMapID'] #PLUTO database ID of some sort\n",
    "#model_data['PolicePrct'] #police precinct\n",
    "#model_data['ProxCode'] #categorical data, proximity to closest house. Doesn't make a diff\n",
    "#model_data['ResArea'] #residential floor area. Lot of zeros. nope\n",
    "#model_data['ResidFAR'] #residential floor area ratio to tax lot area\n",
    "#model_data['SHAPE_Area'] #not sure? \n",
    "#model_data['SHAPE_Leng'] #not sure?\n",
    "#model_data['SanitBoro'] #sanitation borough code for tax lot\n",
    "#model_data['SanitDistr'] #sanitation district code for tax lot\n",
    "#model_data['SchoolDist'] #school district for code tax lot\n",
    "#model_data['TaxMap'] #tax map code for tax block and lot\n",
    "#model_data['Tract2010'] #another census tract code\n",
    "#model_data['UnitsRes'] #residential units in the tax lot, 1% values are 0 - already have it\n",
    "#model_data['UnitsTotal'] #all units in the tax lot. Not sure how it'll be useful.\n",
    "model_data['XCoord'] #location of the lot. Remove zeros and YES\n",
    "model_data['YCoord'] #location of the lot. Remove zeros and YES\n",
    "#model_data['YearAlter1'] #lots of exceptions, but year the buiding was altered. nope\n",
    "#model_data['YearBuilt'] #year built - remove zeros!\n",
    "#model_data['ZipCode'] #zip code for tax lot - categorical\n",
    "#model_data['gross_sqft'] #total gross sqft - same as BldgArea pmuch? Remove those that aren't same?\n",
    "model_data['land_sqft'] #land in sqft. Yep\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commented out lines above are columns we want to drop, and the reason for it is listed next to it. Based on our intuition and knowledge, we have reduced 59 features to 15 useful ones (though we won't keep every one of those in by the end either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "required_columns = ['block','residential_units','total_units','year_built','year_of_sale','AssessLand','AssessTot','BldgArea','BldgDepth','BldgFront','BsmtCode','XCoord','YCoord','land_sqft','sale_price']\n",
    "model_data_stepone = pd.DataFrame()\n",
    "    \n",
    "for col_id in list(model_data):\n",
    "    if col_id in required_columns:\n",
    "        model_data_stepone[col_id] = model_data[col_id]\n",
    "\n",
    "#use tax_class_at_sale to remove all non residential buildings (they have a tax class of 4)\n",
    "model_data_steptwo = model_data_stepone.drop(model_data['tax_class_at_sale'][model_data['tax_class_at_sale'] == 4].index)\n",
    "model_data_steptwo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Outliers by feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to assess the remaining variables - some of them have outliers, some of them have sparse data, and some still have zeros. By plotting the data we can see where exceptional outliers lie, and get rid of these values for our model. NB: this means our predictions are only valid for a certain range of parameters (which we can specify later).\n",
    "\n",
    "Check the rows on a case by case basis:\n",
    "\n",
    "(You can plot each param with/without their constaints to see the effect of outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#land_sqft - get rid of values > 150000\n",
    "#YES year built - impute with median\n",
    "#not residential_units - get rid of values > 100\n",
    "#not tot_units - get rid of values > 100\n",
    "#not BsmtCode, its legit\n",
    "#not BldgFront - get rid of values > 200\n",
    "#BldgDepth - get rid of values > 300\n",
    "#YES XCoord - median the zeros\n",
    "#YES YCoord - median the zeros\n",
    "#block - leave as is\n",
    "#year of sale - leave as is\n",
    "#AssessLand - get rid of values > 3,000,000\n",
    "#AssessTot - leave as is\n",
    "#BldgArea - leave as is\n",
    "#sale_price - get rid of values > 4,000,000 \n",
    "\n",
    "param = model_data_steptwo['BldgFront'][model_data_steptwo['BldgFront'] < 200]\n",
    "print('Rows that will be lost:',(len(model_data_steptwo) - len(param)))\n",
    "#plt.scatter(param.loc[:].index,param.loc[:])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to make a new dataframe with all values that don't fit the constraints to be taken out. There are many approaches for this but we choose to make a new dataframe after checking each row if it fits all the constraints. \n",
    "\n",
    "(To be honest this code is quite slow and can be improved, but for now we'll keep it at this. It prints out the row number every 5000 rows, and there are about 160000 rows altogether. It takes about an hour to run the code block once!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_stepthree = pd.DataFrame(columns=model_data_steptwo.columns.values)\n",
    "\n",
    "print(len(model_data_steptwo))\n",
    "#this is gona take a while.\n",
    "for index, row in model_data_steptwo.iterrows():\n",
    "    if (index%5000 == 0):\n",
    "        print(index)\n",
    "    if (row['residential_units'] < 100):\n",
    "        if (row['total_units'] < 100):\n",
    "            if (row['BldgFront'] < 200):\n",
    "                if (row['BldgDepth'] < 300):\n",
    "                    if (row['AssessLand'] < 3000000):\n",
    "                        if (row['sale_price'] < 4000000):\n",
    "                            if (row['land_sqft'] < 150000):\n",
    "                                model_data_stepthree.loc[index] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to impute zero values in year_built, land_sqft, XCoord and YCoord with the median value from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "med_year = model_data_stepthree['year_built'].median()\n",
    "med_XCoord = model_data_stepthree['XCoord'].median()\n",
    "med_YCoord = model_data_stepthree['YCoord'].median()\n",
    "med_landsqft = model_data_stepthree['land_sqft'].median()\n",
    "\n",
    "model_data_stepfour = model_data_stepthree\n",
    "\n",
    "model_data_stepfour['year_built'].replace(to_replace=0.0,value=med_year,inplace=True)\n",
    "model_data_stepfour['XCoord'].replace(to_replace=0.0,value=med_XCoord,inplace=True)\n",
    "model_data_stepfour['YCoord'].replace(to_replace=0.0,value=med_YCoord,inplace=True)\n",
    "model_data_stepfour['land_sqft'].replace(to_replace=0.0,value=med_landsqft,inplace=True)\n",
    "\n",
    "model_data_stepfour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, split the training data into training and checking model vailidity - we don't have any sale_price values for the test data, so can't internally test our model.\n",
    "\n",
    "model_data is all the data we can use for our training, which we will break up into train/test. The 'test' data in this case is used for internal testing, and the 'valid' data is validation data - it doesn't have values for sale price, so it's not really useful to us I guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model_data_stepfour.sample(frac = 0.8)\n",
    "valid = model_data_stepfour.sample(frac = 0.2)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 14 columns remaining. Let us plot, for each feature, the samples against sale_price, to see their relative performance/their relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head() #from here, we have 72 different features to pick from and one label - sale_price\n",
    "#The first feature we pick can be land_sqft\n",
    "\n",
    "colnames = model_data_stepfour.columns.values\n",
    "feature_colnames = colnames[:len(colnames)-1]\n",
    "label_colname = colnames[len(colnames)-1]\n",
    "\n",
    "results = pd.DataFrame(columns = colnames)\n",
    "results['measurements'] = ['Coeff',\"MSE\",\"Variance\",\"LinearScore\"]\n",
    "results.set_index('measurements',inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(8,15), facecolor='w', edgecolor='k')\n",
    "fignum = 1;\n",
    "\n",
    "for att in feature_colnames:\n",
    "    #for att in feature_colnames:\n",
    "    train_X1 = np.reshape(np.array(train[att]),[len(train),1])\n",
    "    valid_X1 = np.reshape(np.array(valid[att]),[len(valid),1])\n",
    "\n",
    "    train_Y = np.reshape(np.array(train['sale_price']),[len(train),1])\n",
    "    valid_Y = np.reshape(np.array(valid['sale_price']),[len(valid),1])\n",
    "\n",
    "    #create Linear Model\n",
    "    regr = linear_model.LinearRegression(normalize=True)\n",
    "    #fit Linear Model\n",
    "    regr.fit(train_X1, train_Y)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    pred_Y = regr.predict(test_X1)\n",
    "    \n",
    "    results[att].loc['Coeff'] = regr.coef_\n",
    "    results[att].loc['MSE'] = mean_squared_error(test_Y, pred_Y)\n",
    "    results[att].loc['Variance'] = r2_score(test_Y, pred_Y)\n",
    "    results[att].loc['LinearScore'] = regr.score(test_Y, pred_Y)\n",
    "    \n",
    "    ax = fig.add_subplot(7,2,fignum)\n",
    "    ax.scatter(test_X1, test_Y,  color='red', s = 3)\n",
    "    ax.plot(test_X1, pred_Y, color='blue', linewidth = 1)\n",
    "    ax.tick_params(length=2, width=1)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "    ax.set_title(att)\n",
    "    plt.tight_layout()\n",
    "    fignum = fignum + 1\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graphs above (and through trial and error), we decide to remove certain columns, that end up improving the r^2 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_train = train.drop(columns = ['BsmtCode','AssessTot','block','AssessLand','BldgFront','BldgDepth'])\n",
    "new_valid = valid.drop(columns = ['BsmtCode','AssessTot','block','AssessLand','BldgFront','BldgDepth'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a number of different models with our final dataset, and find their respective R2 scores for the training and test dataset. In this way, we can find our most successful technique. For this, we have to import a number of models, then run our data through each model, and finally plot the R2 value for the test data to show its performance against other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import ensemble\n",
    "\n",
    "Xtrn = new_train[new_train.columns.values[:-1]]\n",
    "Ytrn = new_train[label_colname]\n",
    "Xtest = new_test[new_valid.columns.values[:-1]]\n",
    "Ytest = new_test[label_colname]\n",
    "\n",
    "models = [LinearRegression(),\n",
    "          linear_model.Lasso(alpha=0.1),\n",
    "          Ridge(alpha=100.0),\n",
    "          RandomForestRegressor(n_estimators=100, max_features='sqrt'),\n",
    "          KNeighborsRegressor(n_neighbors=10),\n",
    "          DecisionTreeRegressor(max_depth=4),\n",
    "          ensemble.GradientBoostingRegressor()]\n",
    "\n",
    "TestModels = pd.DataFrame()\n",
    "tmp = {}\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    m = str(model)\n",
    "    tmp['Model'] = m[:m.index('(')] #get the chars in the string until char '('. Smart.\n",
    "    model.fit(Xtrn, Ytrn)\n",
    "    tmp['R2_Price'] = r2_score(Ytest, model.predict(Xtest))\n",
    "    print('score on training',model.score(Xtrn, Ytrn))\n",
    "    print('r2 score',r2_score(Ytest, model.predict(Xtest)))\n",
    "    TestModels = TestModels.append([tmp])\n",
    "    \n",
    "TestModels.set_index('Model', inplace=True)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.R2_Price.plot(ax=axes, kind='bar', title='R2_Price')\n",
    "plt.show()\n",
    "\n",
    "TestModels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graph and table above, the best R2 value comes from the RandomForestRegressor. There are minimal signs of overfitting as the training and test scores are both fairly close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poorest performing models are the Linear, Lasso and Ridge regression models. The Lasso and Ridge regression models are quite similar to Linear, but penalise features that have little or no impact on the final prediction. As we have already manually removed features that do not add value, it is not surprising to see that all the regression models have similar performance.\n",
    "\n",
    "KNeigboursRegressor works best at an optimum of about 10 nearest neigbours. Inherently, this model performs poorly on datasets with only a few points, and that too on predictions that involve extrapolation. As this is not the case, the model performs decently but does not capture all the intricacies of the data, and perhaps a weighted approach would work better.\n",
    "\n",
    "GradientBoostingRegressor works quite well, though it is based off a decision tree, and between as a Random Forest is merely a more complicated and larger version of a decision tree, it is understandable why a Random Forest would provide a better approach than a singular decision tree. \n",
    "\n",
    "There are still improvements in accuracy that could be made, with various trial and error appraoches or based on previous heuristics. However, the current result was deemed suitable for the dataset and techniques used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
